# -*- coding: utf-8 -*-
"""Another copy of Copy of CV - Hands On.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OzPDj9DFt0mvLTVRwEaXiXUdczdbFsmz
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# download the dataset from google drive
# the dataset is a zip file containing imagges of faces
# the file id: 1ciXsVwfA6k0--eaF1roQ8CGdAvi2U2DI
import gdown
file_id = '1ciXsVwfA6k0--eaF1roQ8CGdAvi2U2DI'
gdown.download(f"https://drive.google.com/uc?id={file_id}", 'face_recognition.zip', quiet = False)

import zipfile
import os

# extract the zip file
with zipfile.ZipFile('face_recognition.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/face_recognition')

# show the content of the extracted folder
print("The content of extracted folder: ")
print(os.listdir('/content/face_recognition'))
print(os.listdir('/content/face_recognition/face_recognition/dataset/images'))

# load  dataset
# this function loads the image and converts it to grayscale
def load_image(image_path):
  image = cv2.imread(image_path) # read image from path
  if image is None: # check if image is not loaded, print error
    print('Error: could not load image: {image_path}')
    return None, None
  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # convert from BGR to grayscale
  return image, gray # return image and gray image

# load all paths of images in the folder of dataset
# using glob
import glob
image_paths = glob.glob('/content/face_recognition/face_recognition/dataset/images/*/*.jpg')  # all .jpg in sub-folder
for image_path in image_paths:
    image, gray = load_image(image_path)
    if image is not None:
        print(f"Loaded: {image_path}")
    else:
      print(f"Failed to load: {image_path}")

# load Haar Cascade Model for face detection
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# function to detect faces in the image
# using detectMultiScale
''' this function accepts the images in grayscale, with the following parameters:
- scaleFactor: parameter specifying how much the image size is reduced at each image scale
- minNeighbors: parameter specifying how many neighbors each candidate rectangle should have to retain it
- minSize: minimum possible object size'''
def detect_faces(image_gray, scaleFactor=1.05, minNeighbors=4, minSize=(30,30)): 
  faces = face_cascade.detectMultiScale(
      image_gray,
      scaleFactor = scaleFactor,
      minNeighbors = minNeighbors,
      minSize = minSize
  )
  return faces

# load the dataset: list all images and labels folder names
dataset_dir = '/content/face_recognition/face_recognition/dataset/images'
print(f"Dataset directory:  {dataset_dir}")
print(f"Files in dataset directory: {os.listdir(dataset_dir)}")
images = [] # store images in grayscale
labels = [] # stor labels of images

# iterate through the dataset directory
for root, dirs, files in os.walk(dataset_dir):
  if len(files) == 0: # if no files in the directory, continue
    continue
  for f in files: # iterate through the files in the directory
    image_path = os.path.join(root, f) # get the full path of the image
    image, gray = load_image(image_path) # load the image and convert it to grayscale
    if image is not None: # if image is laoded, detect faaces in gray image
      faces = detect_faces(gray)
      if len(faces) > 0: # if faces are detected, save the image and label
        print(f"Faces detected in: {image_path}")
        images.append(gray)
        labels.append(root.split('/')[-1]) # save the label of the image
      else:
        print(f"No faces detected in: {image_path}")
    else:
      print(f"Failed to load: {image_path}")

# cropping faces
'''this function crops the detected face(s) from the original image using
the provided bounding box coordinates'''

def crop_faces(image_gray, faces, return_all=False):
  cropped_faces = [] # save cropped faces
  selected_faces = [] # save coordinates of faces
  if len(faces) > 0: # if faces are detected
    if return_all: # if return_all is true, crop all faces
      for x, y, w, h in faces:
        selected_faces.append((x, y, w, h))
        cropped_faces.append(image_gray[y:y+h, x:x+w])
    else: # if return_all is false, crop the largest face
      x, y, w, h = max(faces, key=lambda rect: rect[2] * rect[3])
      selected_faces.append((x, y, w, h))
      cropped_faces.append(image_gray[y:y+h, x:x+w])
  return cropped_faces, selected_faces

# resize and flatten faces
face_size = (128, 128) # resize to 129=8x128
'''this function resizes the face to the specified size and flattens it to a 1D array'''
def resize_and_flatten(face):
  face_resized = cv2.resize(face, face_size)
  face_flattened = face_resized.flatten()
  return face_flattened

# prepare training and testing data (pre-proessing)
'''remember: iterating through the images, selecting largest face in each image,
resizing and flattening it'''

X = [] # store the images after flattenting
y = [] # store the labels of images

for image, label in zip(images, labels):
  faces = detect_faces(image) # detect face in the image
  cropped_faces, _ = crop_faces(image, faces) # crop the face
  if len(cropped_faces) > 0:
    face_flattened = resize_and_flatten(cropped_faces[0]) # resize and flatten
    X.append(face_flattened)
    y.append(label)

X = np.array(X) # convert to numpy array
y = np.array(y) 

# split the data into training and testing sets
# using stratified sampling to keep the same distribution of labels in both sets 
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 177, stratify = y)

# mean-centering (substracting the mean face)
from sklearn.base import BaseEstimator, TransformerMixin

class MeanCentering(BaseEstimator, TransformerMixin):
  def fit(self, X, y = None):
    self.mean_face = np.mean(X, axis = 0) # calculate the mean face
    return self

  def transform(self, X): # substract the mean face from each image
    return X - self.mean_face

# pipeline: mean clustering -> PCA -> SVM
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.decomposition import PCA

pipe = Pipeline([
    ('centering', MeanCentering()), # 1st step: mean centering
    ('pca', PCA(n_components=13, svd_solver = 'randomized', whiten = True, random_state = 177)), # 2nd step: PCA
    ('svc', SVC(kernel = 'linear', random_state=177)) # 3rd step: SVM
])

# train and evaluate the model
from sklearn.metrics import classification_report

pipe.fit(X_train, y_train)

# evaluate the model
y_pred = pipe.predict(X_test)
print(classification_report(y_test, y_pred))

import numpy as np
import matplotlib.pyplot as plt

# plot explained variance
# to see how many components to keep
pca = pipe[1]  # PCA in my pipeline
plt.figure(figsize=(8,5))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Choosing n_components')
plt.grid()
plt.show()

# visualizing the eigenfaces
n_components = len(pipe[1].components_)

ncol = 4
nrow = (n_components + ncol - 1) // ncol
fig, axes = plt.subplots(nrow, ncol, figsize = (10, 2.5*nrow),
subplot_kw = {'xticks':[], 'yticks':[]})

eigenfaces = pipe[1].components_.reshape((n_components, X_train.shape[1]))
for i, ax in enumerate(axes.flat):
  # check if 'i' is within the bounds of 'eigenfaces'
  if i < n_components:
    ax.imshow(eigenfaces[i].reshape(face_size), cmap='gray') # show the eigenface
    ax.set_title(f'Eigenface {i+1}')
  # hide any extra axes that are not used
  else:
    ax.axis('off')

plt.tight_layout()
plt.show()

# save the model pipeline
import pickle

with open('eigenface_pipeline.pkl', 'wb') as f:
  pickle.dump(pipe, f)

# combine all methods
# calculate the eigenface score
def get_eigenface_score(X):
  X_pca = pipe[:2].transform(X) # apply mean-centering + PCA
  eigenface_scores = pipe[2].decision_function(X_pca) # get the SVM scores
  return eigenface_scores
# predict the label of new image
def eigenface_prediction(image_gray):
  faces = detect_faces(image_gray)
  cropped_faces, selected_faces = crop_faces(image_gray, faces)

  if len(cropped_faces) == 0:
    return 'No face detected'

  X_face = []
  for face in cropped_faces:
    face_flattened = resize_and_flatten(face)
    X_face.append(face_flattened)
    X_face = np.array(X_face)
    labels = pipe.predict(X_face) # predict the label
    scores = get_eigenface_score(X_face) # get the eigenface score

    return scores, labels, selected_faces

# add label and score to the image
import numpy as np

def draw_text(image, label, score,
              font=cv2.FONT_HERSHEY_SIMPLEX,
              pos=(0, 0),
              font_scale=0.6,
              font_thickness=2):
    x, y = pos

    # convert score to confidence (0 - 100%)
    confidence = 1 / (1 + np.exp(-score)) * 100

    # choose color based on confidence
    if confidence > 50:
        text_color = (0, 0, 0)            # black
        text_color_bg = (0, 255, 0)        # green
    else:
        text_color = (255, 255, 255)       # white
        text_color_bg = (0, 0, 255)        # red

    # calculate text size
    confidence_text = f'Conf: {confidence:.1f}%'
    (w1, h1), _ = cv2.getTextSize(confidence_text, font, font_scale, font_thickness)
    (w2, h2), _ = cv2.getTextSize(label, font, font_scale, font_thickness)
    # draw background rectangle
    cv2.rectangle(image, (x, y - h1 - h2 - 25), (x + max(w1, w2) + 20, y), text_color_bg, -1)
    # write lavel and confidence
    cv2.putText(image, label, (x + 10, y - 10), font, font_scale, text_color, font_thickness)
    cv2.putText(image, confidence_text, (x + 10, y - h2 - 15), font, font_scale, text_color, font_thickness)

# draw the result on the image + label + score
# this function draws the bounding box and label on the image
def draw_result(image, scores, labels, coords):
  result_image = image.copy()
  for(x, y, w, h), label, score in zip(coords, labels, scores):
    cv2.rectangle(result_image, (x,y), (x + w, y + h), (0, 255, 0), 2) # draw rectangle around the face
    draw_text(result_image, label, score, pos = (x,y)) # draw label and score
  return result_image

# real-time face recognituon using webcam
print("Starting webcam for real-time face recognition...")

# load the model pipeline trained before
import joblib
model = joblib.load('eigenface_pipeline.pkl')

# open webcam
cap = cv2.VideoCapture(0)

if not cap.isOpened():
    print("Error: Could not open webcam.")
    exit()

while True:
    ret, frame = cap.read()
    if not ret:
        print("Failed to grab frame.")
        break

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # detecting faces and getting predictions
    result = eigenface_prediction(gray)

    if result != 'No face detected':
        scores, labels, coords = result
        frame = draw_result(frame, scores, labels, coords)

    # show the result
    cv2.imshow('Real-time Face Recognition', frame)

    # press 'q' to quit
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# clean up the resources
cap.release()
cv2.destroyAllWindows()